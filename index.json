[{"authors":["admin"],"categories":null,"content":"I am a Data Analyst and researcher in the Systems Neuroscience and Pain Laboratory at the Stanford University School of Medicine. My educational background is in biomedical computation and engineering, and I have experience working in in a variety of fields and research laboratories from clinical informatics with electronic health records to single-cell data analysis and image processing. Currently, I develop machine learning pipelines for finding patterns in EHR to improve patient care, and work on electronic data capture (EDC) tools to improve research infrastructure (among other projects).\nI completed my Master of Science (M.S.) in Biomedical Informatics at Stanford University School of Medicine, and I received my Bachelor’s of Science (B.S.) in Biomedical Computation from the Stanford University School of Engineering. My research interests include biomedical data mining, developing tools to improve bio-imaging, and creating novel machine learning algorithms for the biomedical research space.\nWhile I was an undergraduate, I was a member of Stanford’s NCAA Division I wrestling team, I tutored chemistry courses for underclassmen, and I led Stanford Pre-Orientation Trips (SPOT) for incoming freshmen. I currently help out as an assistant coach for the local high school and middle school wrestling teams, and I enjoy outdoor activities in my spare time. My long term goal is to attend a medical school dual degree program to earn a MD-PhD and continue my explorations in the patient-centered bioinformatics space.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://emcramer.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Data Analyst and researcher in the Systems Neuroscience and Pain Laboratory at the Stanford University School of Medicine. My educational background is in biomedical computation and engineering, and I have experience working in in a variety of fields and research laboratories from clinical informatics with electronic health records to single-cell data analysis and image processing. Currently, I develop machine learning pipelines for finding patterns in EHR to improve patient care, and work on electronic data capture (EDC) tools to improve research infrastructure (among other projects).","tags":null,"title":"Eric Cramer","type":"authors"},{"authors":["Eric Cramer","Luzmercy Perez","Maisa Ziadni","Sean Mackey","Beth Darnall"],"categories":null,"content":"The Stanford Expectations of Treatment Scale (SETS) is a tool developed to measure patient outcome expectancy prior to treatment. It has been validated in patients receiving surgical and pain interventions, but its relationship with expectancies regarding opioid use tapering has not previously been examined. We aim to characterize the relationship between the SETS scores and patient expectancy regarding opioid tapering and pain levels post tapering.\n","date":1619197200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619197200,"objectID":"7f1fc8ce2190d33a0cf41048f1d41053","permalink":"http://emcramer.github.io/talk/aapm2021/","publishdate":"2021-04-23T11:00:00-08:00","relpermalink":"/talk/aapm2021/","section":"talk","summary":"The Stanford Expectations of Treatment Scale (SETS) is a tool developed to measure patient outcome expectancy prior to treatment. It has been validated in patients receiving surgical and pain interventions, but its relationship with expectancies regarding opioid use tapering has not previously been examined. We aim to characterize the relationship between the SETS scores and patient expectancy regarding opioid tapering and pain levels post tapering.","tags":[],"title":"The association of the Stanford Expectations of Treatment Scale (SETS) with expectations on pain and opioid dose in a patient-centered prescription opioid tapering program","type":"talk"},{"authors":["Jennifer M. Hah","Chinwe A. Nwaneshiudu","Eric Cramer","Ian Carroll","Catherine Curtin"],"categories":null,"content":"\r\r","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"d0af6d6800f1d38c036bddd65d9048dc","permalink":"http://emcramer.github.io/publication/hah-2021/","publishdate":"2021-04-18T08:31:23.044679Z","relpermalink":"/publication/hah-2021/","section":"publication","summary":"The prevalence of severe chronic postsurgical pain (CPSP) is 10% across a range of operations, and 22% of patients undergoing hand surgery develop CPSP 1 year after surgery. Identifying an immediate postoperative predictor of remote pain resolution occurring months after surgery has important clinical implications for the management of acute pain and prevention of CPSP. We hypothesized that, by comparing 15 acute pain descriptors among patients undergoing CTR or TFR, worst pain reported on postoperative day (POD) 10 would best predict time to pain resolution. Assessment of pain intensity on POD 10 best predicts remote time to pain resolution in patients undergoing minor hand surgery under local anesthesia. Early identification of patients at high risk for CPSP allows for early intervention, closer follow-up, and initiation of multimodal pain therapy. Average pain intensity reported on POD 10 best predicted time to pain resolution and was significantly associated with the development of CPSP 90 days after surgery. Patients at high risk for the development of CPSP reported average pain on POD 10 ≥ 3.","tags":null,"title":"Acute Pain Predictors of Remote Postoperative Pain Resolution After Hand Surgery","type":"publication"},{"authors":["Kristen Hymel Scherrer","Maisa S. Ziadni","Jiang-Ti Kong","John A. Sturgeon","Vafi Salmasi","Juliette Hong","Eric Cramer","Abby L. Chen","Teresa Pacht","Garrick Olson","Beth D. Darnall","Ming-Chih Kao","Sean Mackey"],"categories":[],"content":"Introduction: Critical for the diagnosis and treatment of chronic pain is the anatomical distribution of pain. Several body maps allow patients to indicate pain areas on paper; however, each has its limitations.\nObjectives: To provide a comprehensive body map that can be universally applied across pain conditions, we developed the electronic Collaborative Health Outcomes Information Registry (CHOIR) self-report body map by performing an environmental scan and assessing existing body maps.\nMethods: After initial validation using a Delphi technique, we compared (1) pain location questionnaire responses of 530 participants with chronic pain with (2) their pain endorsements on the CHOIR body map (CBM) graphic. A subset of participants (n = 278) repeated the survey 1 week later to assess test–retest reliability. Finally, we interviewed a patient cohort from a tertiary pain management clinic (n = 28) to identify reasons for endorsement discordances.\nResults: The intraclass correlation coefficient between the total number of body areas endorsed on the survey and those from the body map was 0.86 and improved to 0.93 at follow-up. The intraclass correlation coefficient of the 2 body map graphics separated by 1 week was 0.93. Further examination demonstrated high consistency between the questionnaire and CBM graphic (\u0026lt;10% discordance) in most body areas except for the back and shoulders (≈15–19% discordance). Participants attributed inconsistencies to misinterpretation of body regions and laterality, the latter of which was addressed by modifying the instructions.\nConclusions: Our data suggest that the CBM is a valid and reliable instrument for assessing the distribution of pain.\n","date":1611535445,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611535445,"objectID":"0dcea4c842db46fa254a5ef6d00e5791","permalink":"http://emcramer.github.io/publication/scherrer-2021/","publishdate":"2021-01-24T16:44:05-08:00","relpermalink":"/publication/scherrer-2021/","section":"publication","summary":"Validation of the CHOIR Body Map (CBM) tool.","tags":[],"title":"Development and validation of the Collaborative Health Outcomes Information Registry body map","type":"publication"},{"authors":["Eric Cramer"],"categories":["projects"],"content":"Tracking the COVID-19 Pandemic After my county instituted a shelter-in-place lockdown due to the COVID-19 pandemic (caused by the SARS-CoV-2 virus), I was curious to know how bad the situation was. The nonpartisan website usafacts.org/ publishes national, state, and county level data for the United States, and provides a substantial amount of information for making your own calculations and conclusions.\nI am particularly interested in the change rate, the change in the number of new cases of COVID-19 per day. This simple calculation gives an estimate of how quickly the virus is spreading, and whether measures put in place to curb the disease are working.\nI intend to add other metrics to this project, such as R0 calculations and forecasting.\n\rOpen the tracking tool in a separate window here. GitHub repository here.\n","date":1585424203,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585424203,"objectID":"494c62d4563ad159ddd6b7413e1db1c0","permalink":"http://emcramer.github.io/project/covid19tracker/","publishdate":"2020-03-28T11:36:43-08:00","relpermalink":"/project/covid19tracker/","section":"project","summary":"A simple tool to calculate county-level change rates during the COVID-19 pandemic.","tags":["bioinformatics","epidemiology"],"title":"COVID-19 Tracker","type":"project"},{"authors":["Eric Cramer"],"categories":[],"content":"In a previous post I talked about adapting a linear regression algorithm so it can be used in a distributed system. Essentially, a master computer oversees computations run on local data, and the algorithm pauses midway through to send summary statistics to the master. In this way, the master receives enough information to reconstruct the model without seeing the underlying data.\nFor a linear regression model, we can simply have the master iteratively pass candidate \\(\\beta\\)s values to to the workers, which then return their local sum of the residual squares. By minimizing the sum of the squared residuals on each iteration, the master can find the optimal values for the \\(\\beta\\)s in \\(y=\\beta_0 + \\beta_1x\\).\nWe can expand this from simple linear regression with a single predictor to multiple linear prediction with several predictors ($y=\\beta_0 + \\beta_1x + \u0026hellip; +\\beta_nx$). The sum of the squared residuals is simply the summary statistic of a matrix operation. Therefore a master controller can pass a vector of \\(\\beta\\) parameters to the workers on each iteration and receive the RSS in return:\n$$RSS_{local}=\\sum_{i=1}^n{\\begin{bmatrix}\rx_{1,1} \u0026amp; ... \u0026amp; x_{1,n} \u0026amp; \\\\ ... \u0026amp; ... \u0026amp; ... \u0026amp; \\\\ x_{m,1} \u0026amp; ... \u0026amp; x_{m,n} \u0026amp; \\end{bmatrix}\\times\\begin{bmatrix}\r\\beta_0 \\\\\r... \\\\\r\\beta_m\r\\end{bmatrix}}$$\nWith a few minor changes in code from my previous post, we can adjust our loss function to accomodate a vector of \\(\\beta\\)s.\n# define a residual sum of squares function to handle multiple sites\rmulti.min.RSS \u0026lt;- function(sites, par){\rrs \u0026lt;- 0\r# calculate the residuals from each data source\rfor(site in sites){\rtmp_mat \u0026lt;-as.matrix(site$data[,1:(ncol(site$data)-2)])\rtmps \u0026lt;- par[1] + tmp_mat%*%par[-1] - site$data$y\rrs \u0026lt;- rs + sum(tmps^2)\r}\r# return the square and sum of the residuals\rreturn(rs)\r}\r Now all we need to do is simulate some multi-variate data and test everything out.\n# general function for simulating a sample data set given parameters\rsim.data \u0026lt;- function(mu, sig, amt, seed, mpar, nl){\r# Simulate data for the practice set.seed(seed)\rx \u0026lt;- replicate(length(mpar)-1, rnorm(n=amt, mean=mu, sd=sig))\r# create the \u0026quot;true\u0026quot; equation for the regression\ra.true \u0026lt;- mpar[-1]\rb.true \u0026lt;- mpar[1]\ry \u0026lt;- x%*%a.true+b.true\r# set the noise level\rnoise \u0026lt;- rnorm(n=amt, mean=0, sd=nl)\rd \u0026lt;- data.frame(x\r, \u0026quot;y_true\u0026quot;=y\r, \u0026quot;y\u0026quot;=y + noise)\rreturn(d)\r}\rtrue_vals \u0026lt;- c(2,4,6,8)\rsim.data1 \u0026lt;-sim.data(10,2,100,2020,true_vals,1)\rsim.data2 \u0026lt;- sim.data(10,2,100,2019,true_vals,1)\rsites \u0026lt;- list(site1 = list(data=sim.data1), site2 = list(data=sim.data2))\rknitr::kable(head(sim.data1))\r    X1 X2 X3 y_true y     10.753944 6.542432 8.540934 152.5978 153.5145   10.603097 8.017478 11.702755 186.1393 185.9124   7.803954 8.828989 9.207017 159.8459 161.0281   7.739188 10.767043 10.813357 184.0659 185.5874   4.406931 11.493330 7.922893 151.9708 153.4088   11.441147 8.143158 7.488237 156.5294 158.8567    knitr::kable(head(sim.data2))\r    X1 X2 X3 y_true y     11.477045 8.309900 11.441690 189.3011 189.1410   8.970479 11.715855 9.210739 181.8630 181.7065   6.719637 8.632787 11.965325 176.3979 177.0496   11.832074 9.978611 7.191396 166.7311 167.2667   7.465036 7.191671 11.600407 167.8134 168.0076   11.476496 12.783553 8.498515 192.5954 192.8948    We can test our loss function with a call to optim and compare the results to the base R linear modeling function (and the true values of our simulation).\nparam.fit \u0026lt;- optim(par=c(0,0,0,0),\rfn = multi.min.RSS,\rhessian = TRUE,\rsites=sites)\r# stack the data frames vertically for later verification\rsim.data3 \u0026lt;- as.data.frame(rbind(sim.data1, sim.data2)) mlm \u0026lt;- lm(y~., data=sim.data3[,-4])\rd \u0026lt;- data.frame(\u0026quot;True Betas\u0026quot;=true_vals\r, \u0026quot;Base R Coefficients\u0026quot;=coef(mlm)\r, \u0026quot;Distributed Coefficients\u0026quot;=param.fit$par)\rknitr::kable(d)\r     True.Betas Base.R.Coefficients Distributed.Coefficients     (Intercept) 2 2.604539 2.642250   X1 4 4.018984 4.021583   X2 6 5.936988 5.935438   X3 8 7.978939 7.973534    Not far off!\nYou can run the full code here.\n","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581724800,"objectID":"e61c1256967535fe96ebb0521560a2ad","permalink":"http://emcramer.github.io/post/multiple-linear-regression-in-a-distributed-system/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/post/multiple-linear-regression-in-a-distributed-system/","section":"post","summary":"In a previous post I talked about adapting a linear regression algorithm so it can be used in a distributed system. Essentially, a master computer oversees computations run on local data, and the algorithm pauses midway through to send summary statistics to the master. In this way, the master receives enough information to reconstruct the model without seeing the underlying data.\nFor a linear regression model, we can simply have the master iteratively pass candidate \\(\\beta\\)s values to to the workers, which then return their local sum of the residual squares.","tags":["computation","R"],"title":"Multiple linear regression in a distributed system","type":"post"},{"authors":["Eric Cramer"],"categories":["research"],"content":"Predicting CRPS Limb Affectation from Physical and Psychological Factors Complex Regional Pain Syndrome (CRPS) is a severe and rare chronic pain condition that often spreads from an initially affected limb to other parts of the body. The underlying etiology and factors that influence the spread of CRPS are not well understood. Previous research has sought to explain the mechanism, onset, and pain severity of CRPS, however, the contribution of psychosocial factors to CRPS affectation has not been investigated fully.\nI extracted data from the Collaborative Health Outcomes Information Registry (CHOIR), which is an electronic patient registry and learning health system. Then I trained a random forest model to describe the role of psychophysical and psychosocial factors as predictors of CRPS limb affectation. To train my model, I defined several \u0026ldquo;classes\u0026rdquo; of CRPS affectation such as ipsilateral and contralateral spread.\nI presented my findings at the American Psychological Association's 126th annual convention in San Francisco, and my presentation received the Society for Health Psychology's Oustanding Poster Presentation award.\n","date":1579652185,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579652185,"objectID":"80ae87819a06226db1d048c0fe8ec0b1","permalink":"http://emcramer.github.io/project/apa2018/","publishdate":"2020-01-21T16:16:25-08:00","relpermalink":"/project/apa2018/","section":"project","summary":"Using machine learning to predict CRPS limb affectation from psychophysical factors.","tags":["bioinformatics","clinical informatics"],"title":"Predicting CRPS Limb Affectation","type":"project"},{"authors":["Eric Cramer"],"categories":["coursework"],"content":"Modeling Calmodulin One of our projects in BMI 214 (Representational Algorithms for Molecular Biology) taught by Dr. Russ Altman, was to create a molecular dynamics simulation of a protein. Molecular Dynamics is using computers to simulate the interaction of atoms and molecules for a period of time under known laws of physics. I represented a fragment of the protein calmodulin by simulating the interactions of its component atoms (each atom within each amino acid). This entails modeling (simulating) how each atom's mass, velocity, energy, and forces change over time, and calculating the effect of the previous moment in time on the subsequent moment.\n","date":1579651403,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651403,"objectID":"7c8302601dd10c19df61444062fde1c0","permalink":"http://emcramer.github.io/project/bmi214/","publishdate":"2020-01-21T16:03:23-08:00","relpermalink":"/project/bmi214/","section":"project","summary":"Using molecular dynamics to model the calmodulin protein.","tags":["bioinformatics","modeling"],"title":"Modeling Calmodulin","type":"project"},{"authors":["Eric Cramer"],"categories":["coursework"],"content":"Class Project For the final project of BMI 273A (The Human Genome Source Code) taught by Dr. Gill Bejarano, I worked with a group to devise a statistic to measure how \u0026ldquo;mutable\u0026rdquo; a given exon is (some mutations impact the function of an exoonic product, while others do not, ie. synonymous vs. nonsynonymous mutations). We used the ExAC dataset to isolate exons and calculate an EMTM score, our version of the RVIS score. We presented our metric, and maintain the results, explorations, and code on GitHub.\n","date":1579635403,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579635403,"objectID":"7c983500b8479eedfd680a453474d80d","permalink":"http://emcramer.github.io/project/bmi273/","publishdate":"2020-01-21T11:36:43-08:00","relpermalink":"/project/bmi273/","section":"project","summary":"Developing a metric to measure how mutable a given human exon may be.","tags":["genomics","bioinformatics"],"title":"Exon Mutability Score","type":"project"},{"authors":[],"categories":[],"content":"\rQuick Intro\rAs hospitals, care providers, and private companies collect more data, they develop rich databases that can be used to improve patient care (e.g. through precision medicine). Research institutions often cannot share their data with each other, however, out of privacy concerns and HIPAA compliance. This poses a hurdle to inter-institutional collaboration, and creates a research bottleneck. It is an unfortunate instance where good data security practices can create roadblocks to inter-institutional collaboration, which has the potential to solve problems such as bias in AI.\nOne way we may circumvent this issue is with distributed computation. Through an appropriately configured distributed computing service, it is possible to fit models on data that match by stacking vertically, but is otherwise electronically separate.\nPartitioning data\n\rThe underlying premise is that most (all?) computations for modelling require multiple steps. If we take values calculated during an intermediate step of a computation performed at individual sites, then we can aggregate these values in a central location to produce a final model. Since the sites don’t talk to each other, and the central location only receives a summary statistic, none of the underlying information gets shared. Each institution or entity can hold on to its data and maintain its security while still helping the common good.\nThat is the vision and purpose of the distcomp R package, which makes the distributed computation process simpler through a series of GUIs that walk a user through the process. The only hang up is there are currently very few options for computations optimized for this distribution process.\nThat is why in this post, I am going to go through prototyping a distributed computation before adding it to the distcomp package. I am going to start small, by adding a linear regression (which was not included in the initial list of possible distributed computations).\n\rPrototyping Locally\rTo do a proper distributed computation, you need to have multiple sites and a master controlling instance. This involves configuring a server or VM. But you don’t really need to do that to prototype a computation and make sure you can integrate values at some intermediate step.\nConsider computing the linear regression for some data set by minimizing the residual sum of squares. Given some set of data points \\((x_i, y_i), i=1,...,n\\), we obtain a residual (error) value in prediction with a model \\(r_i = f(x_i, \\beta)\\). If this model is linear, \\(f(x) = \\beta_1 x + \\beta_0\\), then we can optimize it by minimizing the sum of the residuals: \\(\\sum_{i=1}^n r_i^2\\).\nIt is at this step that we can split up the computation. We have the master send out the parameters (e.g. \\(\\beta_0, \\beta_1\\), etc.) for the computation to each of the participating sites. I can re-create this scenario locally by simulating two separate data sets.\n# general function for simulating a sample data set given parameters\rsim.data \u0026lt;- function(mu, sig, amt, seed, mpar, nl){\r# Simulate data for the practice set.seed(seed)\rx \u0026lt;- rnorm(n=amt, mean=mu, sd=sig)\r# create the \u0026quot;true\u0026quot; equation for the regression\ra.true \u0026lt;- mpar[1]\rb.true \u0026lt;- mpar[2]\ry \u0026lt;- x*a.true+b.true\r# set the noise level\rnoise \u0026lt;- rnorm(n=amt, mean=0, sd=nl)\rd \u0026lt;- cbind(x,y,y + noise)\rcolnames(d) \u0026lt;- c(\u0026quot;x\u0026quot;, \u0026quot;y_true\u0026quot;,\u0026quot;y\u0026quot;)\rreturn(as.data.frame(d))\r}\rsim.data1 \u0026lt;-sim.data(10,2,100,2020,c(2,8),1)\rsim.data2 \u0026lt;- sim.data(10,2,100,2019,c(2,8),1)\rsites \u0026lt;- list(site1 = list(data=sim.data1), site2 = list(data=sim.data2))\rhead(sim.data1)\r## x y_true y\r## 1 10.753944 29.50789 27.77910\r## 2 10.603097 29.20619 28.21493\r## 3 7.803954 23.60791 23.02240\r## 4 7.739188 23.47838 23.86190\r## 5 4.406931 16.81386 17.56053\r## 6 11.441147 30.88229 29.95387\rhead(sim.data2)\r## x y_true y\r## 1 11.477045 30.95409 30.10904\r## 2 8.970479 25.94096 26.79889\r## 3 6.719637 21.43927 20.75567\r## 4 11.832074 31.66415 31.65345\r## 5 7.465036 22.93007 21.52591\r## 6 11.476496 30.95299 32.34477\rHere we simulate two data sets with 100 observations, a mean of 10, and a standard deviation of 2. The true values for the linear model are a slope of 2 and an intercept of 8.\nThen each site will calculate the residuals from its own data and send back the summary statistic - the sum of its squared residuals.\n# define a residual sum of squares function to handle multiple sites\rmulti.min.RSS \u0026lt;- function(sites, par){\rrs \u0026lt;- 0\r# calculate the residuals from each data source\rfor(site in sites){\rtmps \u0026lt;- par[1] + par[2] * site$data$x - site$data$y\rrs \u0026lt;- rs + sum(tmps^2) #c(rs, tmps)\r}\r# return the square and sum of the residuals\rreturn(rs)\r}\rAll that is left to do is solve for each site. We can use base R’s optim function to do this.\nparam.fit \u0026lt;- optim(par=c(0,1),\rfn = multi.min.RSS,\rhessian = TRUE,\rsites=sites)\rprint(\u0026quot;Distributed linear model results:\u0026quot;)\r## [1] \u0026quot;Distributed linear model results:\u0026quot;\rprint(paste(\u0026quot;Intercept: \u0026quot;, param.fit$par[1], \u0026quot; Slope: \u0026quot;, param.fit$par[2]))\r## [1] \u0026quot;Intercept: 7.77183635600249 Slope: 2.00840909246344\u0026quot;\rWe can compare the result’s to R’s built-in linear model function, lm by stacking the data from the two “sites” and running a linear model on the full data set.\n# stack the data frames vertically for later verification\rsim.data3 \u0026lt;- as.data.frame(rbind(sim.data1, sim.data2)) print(\u0026quot;Base R linear model on the full data set:\u0026quot;)\r## [1] \u0026quot;Base R linear model on the full data set:\u0026quot;\rlm(y~x, data=sim.data3)\r## ## Call:\r## lm(formula = y ~ x, data = sim.data3)\r## ## Coefficients:\r## (Intercept) x ## 7.776 2.008\rPretty similar!\nClick here to run the code.\n\r","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579219061,"objectID":"654b09260cef5009009dbdf7a298bc2e","permalink":"http://emcramer.github.io/post/starting-distributed-computing/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/starting-distributed-computing/","section":"post","summary":"Quick Intro\rAs hospitals, care providers, and private companies collect more data, they develop rich databases that can be used to improve patient care (e.g. through precision medicine). Research institutions often cannot share their data with each other, however, out of privacy concerns and HIPAA compliance. This poses a hurdle to inter-institutional collaboration, and creates a research bottleneck. It is an unfortunate instance where good data security practices can create roadblocks to inter-institutional collaboration, which has the potential to solve problems such as bias in AI.","tags":["computation","R"],"title":"Starting distributed computing","type":"post"},{"authors":["Jennifer M. Hah","Eric Cramer","Heather Hilmoe","Peter Schmidt","Rebecca McCue","Jodie Trafton","Debra Clay","Yasamin Sharifzadeh","Gabriela Ruchelli","Stuart Goodman","James Huddleston","William J. Maloney","Frederick M. Dirbas","Joseph Shrager","John G. Costouros","Catherine Curtin","Sean C. Mackey","Ian Carroll"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"a5db005ef731d7483cbe9001c5f99f12","permalink":"http://emcramer.github.io/publication/hah-2019/","publishdate":"2020-01-12T08:31:23.044679Z","relpermalink":"/publication/hah-2019/","section":"publication","summary":"","tags":null,"title":"Factors Associated With Acute Pain Estimation,  Postoperative Pain Resolution,  Opioid Cessation,  and Recovery","type":"publication"},{"authors":["Eric M. Cramer","Martin G. Seneviratne","Husham Sharifi","Alp Ozturk","Tina Hernandez-Boussard"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a1ddfb50c4a3aea09ac4e5352b2da694","permalink":"http://emcramer.github.io/publication/cramer-2019/","publishdate":"2020-01-12T08:31:23.081984Z","relpermalink":"/publication/cramer-2019/","section":"publication","summary":"","tags":null,"title":"Predicting the Incidence of Pressure Ulcers in the Intensive Care Unit Using Machine Learning","type":"publication"},{"authors":["E. Cramer","M. Ziadni","K. Scherrer","S. Mackey"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"1252059ce83a901439e9e4a6e2654560","permalink":"http://emcramer.github.io/publication/cramer-2018/","publishdate":"2020-01-12T08:31:23.11144Z","relpermalink":"/publication/cramer-2018/","section":"publication","summary":"","tags":null,"title":"The somatic distribution of chronic pain and emotional distress utilizing the collaborative health outcomes information registry (CHOIR) bodymap","type":"publication"}]